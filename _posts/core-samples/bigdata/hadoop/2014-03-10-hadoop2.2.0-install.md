---
layout: post
title: "hadoop2.2.0 集群安装 ubuntu12"
category : hadoop
tags : [bigData, hadoop]
---
author:xiajun

	写这篇文章主要是记录一下自己的安装过程和踩下的坑
2.2.0的安装和1.x的安装前奏是一样的，jdk,免密钥登录等，可以参考1.1.0安装文章

安装：

1.下载hadoop2.2.0版本

[http://apache.fayea.com/apache-mirror/hadoop/common/stable/](http://apache.fayea.com/apache-mirror/hadoop/common/stable/ "hadoop2.2.0")(注意：如果你是在32位机器上安装可以下载编译好的文件hadoop-2.2.0.tar.gz，如果是64位的很不幸你需要下载源码包自己编译)

2.编译

如果你没有安装maven请先行安装并注意版本最好是3.0.5，同时jdk最好是1.6版本否则编译时会有很多问题

在编译hadoop时由于需要使用protobuf2.5和cmake

protobuf2.5安装:
	
	tar -zxf protobuf-2.5.0.tar.gz //版本最好是2.5
	cd protobuf-2.5.0
	./configure --prefix=/usr   //注意：如果是ubuntu 就必须安装在/usr目录
	make check
	make install

cmake安装：

	sudo apt-get install cmake  //ubuntu下这样安装

开始安装hadoop

解压hadoop2.2.0的压缩包，进入 hadoop-2.2.0-src

	mvn package -Pdist,native -DskipTests -Dtar //执行后等待10分钟左右

编译完成后文件存放在 hadoop-2.2.0-src/hadoop-dist/target/hadoop-2.2.0.tar.gz

3.安装

将编译后的文件拷贝到服务器上解压

	tar -zvxf hadoop-2.2.0.tar.gz
	cd hadoop-2.2.0/etc/hadoop


<?prettify lang=xml linenums=true?>
<pre class="prettyprint linenums" id="quine" style="border:4px solid #88c">
<xmp>
	修改：core-site.xml

	<configuration>
	<property>
            <name>fs.defaultFS</name>
            <value>hdfs://master:9000</value>
        </property>
        <property>
            <name>io.file.buffer.size</name>
            <value>131072</value>
        </property>
        <property>
            <name>hadoop.tmp.dir</name>
            <value>/usr/local/jd/hadoop-2.2.0/tmp</value>
            <description>A base for other temporarydirectories.</description>
        </property>
        <property>
            <name>hadoop.proxyuser.root.hosts</name>
            <value>master</value>
        </property>
        <property>
            <name>hadoop.proxyuser.root.groups</name>
            <value>*</value>
        </property>
	</configuration>

	修改：hdfs-site.xml

	<configuration>
	<property>
            <name>dfs.namenode.name.dir</name>
            <value>/usr/local/jd/hadoop-2.2.0/namenode</value>
        </property>
        <property>
            <name>dfs.datanode.data.dir</name>
            <value>/usr/local/jd/hadoop-2.2.0/data</value>
        </property>
        <property>
            <name>dfs.replication</name>
        <value>2</value>
        </property>
        <property>
            <name>dfs.permissions</name>
            <value>false</value>
        </property> 
	</configuration>

	修改：yarn-site.xml

	<configuration>
	<property>
        <name>yarn.resourcemanager.address</name>
        <value>master:18040</value>
    </property>
    <property>
        <name>yarn.resourcemanager.scheduler.address</name>
        <value>master:18030</value>
    </property>
    <property>
        <name>yarn.resourcemanager.resource-tracker.address</name>
        <value>master:18025</value>
    </property>
    <property>
        <name>yarn.resourcemanager.admin.address</name>
        <value>master:18041</value>
    </property>
    <property>
         <name>yarn.resourcemanager.webapp.address</name>
         <value>master:8088</value>
    </property>
    <property>
        <name>yarn.nodemanager.local-dirs</name>
        <value>/usr/local/jd/hadoop-2.2.0/my</value>
    </property>
    <property>
		<name>yarn.nodemanager.log-dirs</name>
		<value>/usr/local/jd/hadoop-2.2.0/logs</value>
    </property>
    <property>
		<name>yarn.nodemanager.log.retain-seconds</name>
		<value>10800</value>
	</property>
	<property>
		<name>yarn.nodemanager.remote-app-log-dir</name>
		<value>/logs</value>
	</property>
	<property>
		<name>yarn.nodemanager.remote-app-log-dir-suffix</name>
		<value>logs</value>
	</property>
	<property>
		<name>yarn.log-aggregation.retain-seconds</name>
		<value>-1</value>
	</property>
	<property>
		<name>yarn.log-aggregation.retain-check-interval-seconds</name>
		<value>-1</value>
	</property>
	<property>
		<name>yarn.nodemanager.aux-services</name>
		<value>mapreduce_shuffle</value>
	</property> 
	<property>
		<name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
		<value>org.apache.hadoop.mapred.ShuffleHandler</value>
	</property>
	</configuration>

	修改：mapred-site.xml

	<configuration>
	<property>
            <name>mapreduce.framework.name</name>
            <value>yarn</value>
        </property>
        <property>
            <name>mapreduce.jobhistory.address</name>
            <value>master:10020</value>
        </property>
        <property>
            <name>mapreduce.jobhistory.webapp.address</name>
            <value>master:19888</value>
        </property>
        <property>
                <name>mapreduce.jobhistory.intermediate-done-dir</name>
                <value>/mr-history/tmp</value>
        </property>
        <property>
            <name>mapreduce.jobhistory.done-dir</name>
            <value>/mr-history/done</value>
        </property> 
	</configuration>
</xmp>
</pre>

修改yarn-env.sh

	export JAVA_HOME=/usr/local/jd/jdk1.6.0_25

修改hadoop-env.sh

	export JAVA_HOME=/usr/local/jd/jdk1.6.0_25
	export HADOOP_HOME=/usr/local/jd/hadoop-2.2.0

配置环境变量
	
	
	export JAVA_HOME=/usr/local/jd/jdk1.6.0_25
	export PATH=$JAVA_HOME/bin:$PATH
	
	export HADOOP_HOME=/usr/local/jd/hadoop-2.2.0
	export HADOOP_COMMON_HOME=$HADOOP_HOME
	export HADOOP_HDFS_HOME=$HADOOP_HOME
	export HADOOP_MAPRED_HOME=$HADOOP_HOME
	export HADOOP_YARN_HOME=$HADOOP_HOME
	export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
	export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HADOOP_HOME/lib
	export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"
	export JAVA_LIBRARY_PATH=$HADOOP_HOME/lib/native

启动：

	./sbin/start-all.sh 

	http://master:8088  查看运行job
	http://master:50070 查看datanode